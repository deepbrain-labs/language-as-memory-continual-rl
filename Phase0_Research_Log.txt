Phase 0 — Research Log
Project: Continual RL — Phase-0 (Symbolic GridWorld Sanity)


---
SUMMARY
---
This document records the exact, reproducible steps performed to reconstruct Phase-0 of the Continual RL project in a fresh Colab notebook. The goal of Phase-0 was to implement a deterministic minimal environment, a symbolic planner that yields the single milestone plan [('goto', 'green', 'goal')], a correct low-level executor that attains total reward 1, and a formal reachability check (BFS). The notebook also contains diagnostics on the effect of random plans vs random low-level actions.

The log below lists the step-by-step actions, exact verification outputs observed during the Colab session, key experimental diagnostics, interpretation, and recommended next steps.

---
ENVIRONMENT & SETUP
---
Google Colab (fresh notebook). Google Drive mounted and canonical project directory created at:
/content/drive/MyDrive/Continual_RL_Phi2

Directory layout (verified):
/content/drive/MyDrive/Continual_RL_Phi2
  checkpoints
  configs
  data
  experiments
  logs
  models
  notebooks
  src
  utils

Core libraries installed and verified (Step 2):
(Exact printed verification output observed in Colab)
--- Verification ---
Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]
torch: 2.9.0+cu126  | cuda_available: True
torch device name: Tesla T4
gymnasium: 1.2.2
stable-baselines3: 2.7.1
numpy: 2.0.2
matplotlib: 3.10.0
pandas: 2.2.2
seaborn: 0.13.2

Smoke test: CartPole-v1 step OK. sample_action: 0 reward: 1.0 terminated: False truncated: False

(Notes)
- Some benign DeprecationWarnings were printed by the Colab runtime; these do not affect reproducibility for Phase-0.
- The GPU available in the session was Tesla T4.

---
PHASE-0 CODE AND RESULTS (step-by-step)
---
Step 1 — Drive mount & canonical project folders
- Action: Mounted Google Drive and created directories under /content/drive/MyDrive/Continual_RL_Phi2

Created directory: /content/drive/MyDrive/Continual_RL_Phi2

Step 2 — Core dependency installation and smoke test
- Action: Installed packages (gymnasium, stable-baselines3, torch, torchvision, torchaudio, matplotlib, pandas, seaborn).
- Verification: Printed versions and ran a CartPole-v1 step smoke test.
- Exact printed verification output is recorded above (see CORE LIBRARIES).

Step 3 — Deterministic SimpleGridEnv implementation
- Action: Implemented `SimpleGridEnv` (fixed start at (0,0); fixed goal at (width-1,height-1); deterministic step; reward = 1 iff agent_pos == goal_pos; episode terminates on success).
- Sanity test executed moving deterministically to the goal.
- Exact printed output (user-provided):
Initial obs: {'agent_pos': (0, 0), 'goal_pos': (3, 3)}
Final obs: {'agent_pos': (3, 3), 'goal_pos': (3, 3)}
Total reward: 1

Interpretation: Environment is deterministic; reward is sparse and terminal; no stochastic reward process is present in this implementation.

Step 4 — Minimal symbolic planner and executor
- Action: Implemented `symbolic_plan(obs)` that returns the single high-level plan: [("goto","green","goal")] and an `execute_plan(env, plan)` low-level executor that greedily moves to the goal.
- Exact printed output (user-provided):
Final Plan: [('goto', 'green', 'goal')]
Total Reward: 1

Interpretation: The symbolic planner emits one symbolic operator. The executor deterministically maps that symbol to a greedy low-level action sequence that reaches the goal and yields reward 1. This is an oracle-grounded executor for Phase-0.

Step 5 — Formal reachability check (BFS)
- Action: Implemented a deterministic breadth-first search reachability check on the grid.
- Exact printed output (user-provided):
Reachable: True

Interpretation: The goal is reachable from the fixed start under the grid adjacency model; BFS returns True as expected.

Step 6 — Diagnostics: random plan vs random low-level action experiments
- Experimental diagnostics run by the user (Monte Carlo style):
Random low-level sequences success rate (seq_len=6): 0.0036
Random symbolic plans success rate (naive grounding): 0.5355

Exact outputs copied verbatim above.

Interpretation of diagnostics:
- Random low-level action sequences (length 6) have very low probability of accidentally reaching the goal (~0.36%). This matches combinatorial expectations because |A|^L grows quickly and only a small fraction of sequences correspond to valid goal-reaching trajectories.
- Random symbolic plans (sampled from a small symbol set and grounded such that presence of ('goto','green','goal') triggers the oracle executor) yield ~53.55% success because roughly half the sampled symbolic plans contained that exact operator. The high success rate here is driven by the sampling distribution over a small symbolic set and by the oracle grounding mechanism — not by the agent learning competence.
- Conclusion: reward is deterministic and trajectory-dependent; the stochasticity observed in success rates stems from the sampling processes (random low-level action sampling vs. random symbolic emission), not from an underlying probabilistic reward mechanism.

---
KEY OBSERVATIONS & RESEARCH-CRITICAL FINDINGS
---
1. Deterministic reward semantics
   - The reward function implemented is strictly `r = 1 iff agent_pos == goal_pos; else r = 0`.
   - No probabilistic reward noise or partial-credit reward was present in Phase-0.
2. Oracle grounding creates misleading success metrics
   - The current executor acts as an oracle: whenever the correct symbol appears the low-level executor deterministically achieves the goal. This inflates apparent success when measuring symbol emission frequency as a proxy for competence.
3. Random low-level vs. random symbolic sampling
   - Random low-level actions are a proper lower-bound baseline, but they are extremely inefficient for learning goal-directed behavior in sparse reward settings.
   - Random symbolic sampling, combined with an oracle executor, is a diagnostic but cannot be interpreted as a learned behavior baseline.
4. Formal reachability is implemented and passes — removes an entire class of silent failure modes in future experiments.
5. This Phase-0 notebook is reproducible: directory layout, package versions, and outputs are recorded above.

---
RESEARCH-QUALITY RECOMMENDATIONS (next experimental directions)
---
To convert Phase-0 into a rigorous experimental baseline and to avoid desk-rejection in top-tier venues, adopt one of the following (pick one experimentally and report results):

A) Imperfect / learned grounding (recommended)
   - Replace the oracle deterministic executor with a learned low-level controller or a stochastic controller.
   - Example: train a small PPO agent conditioned on the symbolic operator token to perform the corresponding low-level actions; or add action noise so that `goto` must be robustly learned.
   - Measure success rates of symbolic-emitting policies where the executor has independent failure probability; this separates symbol correctness from execution competence.

B) Policy-level symbol generation
   - Make the symbolic planner the target of learning: an RL agent (PPO / DQN) outputs symbolic tokens (discrete action set of symbols). The low-level executor should be a fixed learned controller (trained once or concurrently). This tests whether a policy can learn to emit the correct symbol rather than relying on random sampling.

C) Curriculum & structured randomness
   - Use structured randomness (biased sampling toward goal region) and curriculum learning to mitigate sparse reward issues for low-level exploration baselines.

D) Diagnostic control experiment (if you only need an illustrative baseline)
   - Keep the Phase-0 oracle architecture as a locked control and use it only as a diagnostic baseline. Do not treat symbolic sampling frequency as a learning outcome in papers; instead, report it under "oracle baselines / ablations".

Suggested immediate experiments to run (single-cell experiments):
1) Monte Carlo estimate of success when grounding is made stochastic: modify executor so that each low-level step fails with probability p (or the whole `goto` completion succeeds with probability p). Report success-vs-p curve.
2) Train a short-horizon PPO on low-level actions for this 4x4 grid to compare against random low-level baseline and oracle executor.
3) Train a discrete symbolic policy (action space = {emit noop, emit goto_green_goal, emit goto_red_goal, ...}) with a fixed learned low-level controller to measure how symbolic learning curves behave when grounding is non-oracular.

---
EXACT REPRODUCIBILITY NOTES
---
Include these exact snippets in your reproducibility appendix for any paper or supplementary materials:

- Deterministic reward function (pseudo-code):
  reward = 1 if agent_pos == goal_pos else 0

- Symbolic plan (Phase-0):
  [('goto', 'green', 'goal')]

- Environment size used in primary sanity tests: width=4, height=4

- Important observed experimental outputs (verbatim):
  Initial obs: {'agent_pos': (0, 0), 'goal_pos': (3, 3)}
  Final obs: {'agent_pos': (3, 3), 'goal_pos': (3, 3)}
  Total reward: 1
  Final Plan: [('goto', 'green', 'goal')]
  Total Reward: 1
  Reachable: True
  Random low-level sequences success rate (seq_len=6): 0.0036
  Random symbolic plans success rate (naive grounding): 0.5355

---
CONCLUSION
---
Phase-0 is a correct, deterministic implementation of the symbolic GridWorld sanity environment and oracle execution pipeline. The primary scientific caveat is the oracle grounding: symbolic emission probability cannot be conflated with agent competence while the executor deterministically ensures success. To be research-grade for a top-tier venue we should either remove the oracle assumption (learn or corrupt grounding) or treat the current setup strictly as an oracle baseline and perform additional experiments where competence must be earned via learning.

---
END